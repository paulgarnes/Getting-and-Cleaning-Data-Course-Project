}
x = rbind(c(1, -1/4), c(-1/4, 1))
m = makeCacheMatrix(x)
makeVector <- function(x = numeric()) {
m <- NULL
set <- function(y) {
x <<- y
m <<- NULL
}
get <- function() x
setmean <- function(mean) m <<- mean
getmean <- function() m
list(set = set, get = get,
setmean = setmean,
getmean = getmean)
}
m = makeCacheMatrix(x)
cacheSolve <- function(x, ...) {
inv <- x$getinverse()
if(!is.null(inv)) {
message("getting cached data.")
return(inv)
}
data <- x$get()
inv <- solve(data)
x$setinverse(inv)
inv
}
makeCacheMatrix <- function(x = matrix()) {
inv <- NULL
set <- function(y) {
x <<- y
inv <<- NULL
}
get <- function() x
setinverse <- function(inverse) inv <<- inverse
getinverse <- function() inv
list(set=set, get=get, setinverse=setinverse, getinverse=getinverse)
}
ls()
View(x)
makeCacheMatrix <- function(x = matrix()) {
inv <- NULL
set <- function(y) {
x <<- y
inv <<- NULL
}
get <- function() x
setinverse <- function(inverse) inv <<- inverse
getinverse <- function() inv
list(set=set, get=get, setinverse=setinverse, getinverse=getinverse)
}
ls()
cacheSolve <- function(x, ...) {
inv <- x$getinverse()
if(!is.null(inv)) {
message("getting cached data.")
return(inv)
}
data <- x$get()
inv <- solve(data)
x$setinverse(inv)
inv
}
makeCacheMatrix(x)
## Caching the Inverse of a Matrix:
## Matrix inversion is usually a costly computation and there may be some
## benefit to caching the inverse of a matrix rather than compute it repeatedly.
## Below are a pair of functions that are used to create a special object that
## stores a matrix and caches its inverse.
## This function creates a special "matrix" object that can cache its inverse.
makeCacheMatrix <- function(x = matrix()) {
inv <- NULL
set <- function(y) {
x <<- y
inv <<- NULL
}
get <- function() x
setInverse <- function(inverse) inv <<- inverse
getInverse <- function() inv
list(set = set,
get = get,
setInverse = setInverse,
getInverse = getInverse)
}
## This function computes the inverse of the special "matrix" created by
## makeCacheMatrix above. If the inverse has already been calculated (and the
## matrix has not changed), then it should retrieve the inverse from the cache.
cacheSolve <- function(x, ...) {
## Return a matrix that is the inverse of 'x'
inv <- x$getInverse()
if (!is.null(inv)) {
message("getting cached data")
return(inv)
}
mat <- x$get()
inv <- solve(mat, ...)
x$setInverse(inv)
inv
}
my_matrix <- makeCacheMatrix(matrix(1:4, 2, 2))
my_matrix$get()
my_matrix$getInverse()
cacheSolve(my_matrix)
cacheSolve(my_matrix)
## Programming Assignment 2 - Caching the Inverse of a Matrix
## Caching allows us to reclaim valuable computational time especially with
## typically time-intensive procedures like Matrix inversion. That way, there
## is no need to compute the result for the same matrix over and over--provided,
## of course, that the matrix stays the same.It's like visiting the same web-
## page over and over. The page loads faster usually because it is pulling from
## the cache as opposed to the
## The idea of this exercise is to develop a couple functions that not only
## store a matrix but also caches its result..
## The first function, makeCacheMatrix creates an object that can cache its
## inverse according to the steps outlined in the assignment guide:
## 1. Set the value of the matrix
## 2. Get the value of the matrix
## 3. Set the value of the inverse
## 4. Get the value of the inverse
makeCacheMatrix <- function(x = matrix()) {
# store the cached inverse matrix
inv <- NULL
# 1. Set the value of the matrix
set <- function(y) {
x <<- y
inv <<- NULL
}
# 2. Get the value of the matrix
get <- function() x
# 3. Set the value of the inverse
setInverse <- function(inverse) inv <<- inverse
# 4. Get the value of the inverse
getInverse <- function() inv
list(set = set, get = get,
setInverse = setInverse, getInverse = getInverse)
}
## This second function, cacheSolve computes the inverse of the special "matrix"
## from the makeCacheMatrix above. We'll know if this actually works if when the
## matrix has not changed, the function pulls from the cache instead of
## computing the answer from scratch again.
cacheSolve <- function(x, ...) {
inv <- x$getInverse()
if (!is.null(inv)) {
message("getting cached data")
return(inv)
}
mat <- x$get()
inv <- solve(mat, ...)
x$setInverse(inv)
## Return a matrix that is the inverse of 'x'
inv
}
x <- matrix(rnorm(16), nrow = 4)
cx <- makeCacheMatrix(x)
cx$get()
cacheSolve(cx)
cacheSolve(cx)
cacheSolve(cx)
cacheSolve(cx)
## Programming Assignment 2 - Caching the Inverse of a Matrix
## Caching allows us to reclaim valuable computational time especially with
## typically time-intensive procedures like Matrix inversion. That way, there
## is no need to compute the result for the same matrix over and over--provided,
## of course, that the matrix stays the same.It's like visiting the same web-
## page over and over. The page loads faster usually because it is pulling from
## the cache as opposed to the
## The idea of this exercise is to develop a couple functions that not only
## store a matrix but also caches its result..
## The first function, makeCacheMatrix creates an object that can cache its
## inverse according to the steps outlined in the assignment guide:
## 1. Set the value of the matrix
## 2. Get the value of the matrix
## 3. Set the value of the inverse
## 4. Get the value of the inverse
makeCacheMatrix <- function(x = matrix()) {
# store the cached inverse matrix
inv <- NULL
# 1. Set the value of the matrix
set <- function(y) {
x <<- y
inv <<- NULL
}
# 2. Get the value of the matrix
get <- function() x
# 3. Set the value of the inverse
setInverse <- function(inverse) inv <<- inverse
# 4. Get the value of the inverse
getInverse <- function() inv
list(set = set, get = get,
setInverse = setInverse, getInverse = getInverse)
}
## This second function, cacheSolve computes the inverse of the special "matrix"
## from the makeCacheMatrix above. We'll know if this actually works if when the
## matrix has not changed, the function pulls from the cache instead of
## computing the answer from scratch again.
cacheSolve <- function(x, ...) {
inv <- x$getInverse()
if (!is.null(inv)) {
message("Utilizing cached data instead...")
return(inv)
}
mat <- x$get()
inv <- solve(mat, ...)
x$setInverse(inv)
## Return a matrix that is the inverse of 'x'
inv
}
cacheSolve(cx)
set.seed(1)
rpois(5, 2)
set.seed(10)
x <- rep(0:1, each = 5)
e <- rnorm(10, 0, 20)
y <- 0.5 + 2 * x + e
library(datasets)
Rprof()
fit <- lm(y ~ x1 + x2)
Rprof(NULL)
x1 <- 9
library(datasets)
Rprof()
fit <- lm(y ~ x1 + x2)
Rprof(NULL)
x2 <- 8
library(datasets)
Rprof()
fit <- lm(y ~ x1 + x2)
Rprof(NULL)
?qbinom
?rbinom
library(swirl)
rm(list=ls())
library(swirl)
set.seed(13435)
x <- data.frame("var1"=sample(1:5),"var2"+sample(6:10),"var3"=sample(11:15))
x <- data.frame("var1"=sample(1:5),"var2"=sample(6:10),"var3"=sample(11:15))
x <- x[sample(1:5),]; x$cvar2[c(1,3)]=NA
x <- x[sample(1:5),]; x$var2[c(1,3)]=NA
x
x[,1]
x[,"var3"]
x[1:2,"var2"]
x[1:3,"var2"]
x[which(x$var2 > 8)]
x[which(x$var2 > 8),]
x[which(x$var2 >= 8),]
library(plyr)
arrange(x,var1)
x$var4 <- rnorm(5)
x
y <- cbind(x,rnorm(5))
y
if(!file.exists("./data")){dir.create("./data")}
fileUrl <- "https://data.baltimorecity.gov/api/views/k5ry-ef3g/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/restaurants.csv",method="curl")
download.file(fileUrl,destfile="./data/restaurants.csv",method="curl")
restData <- read.csv("./data/restaurants.csv")
if(!file.exists("./data")){dir.create("./data")}
fileUrl <- "https://data.baltimorecity.gov/api/views/k5ry-ef3g/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/restaurants.csv",method="curl")
chicago <- readRDS("chicago.rds")
which(agricultureLogical)[1:3]
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
f <- file.path(getwd(), "ss06hid.csv")
download.file(url, f)
dt <- data.table(read.csv(f))
agricultureLogical <- dt$ACR == 3 & dt$AGS == 6
which(agricultureLogical)[1:3]
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
f <- file.path(getwd(), "ss06hid.csv")
download.file(url, f)
dt <- data.table(read.csv(f))
?data.table
library(dplyr)
dplyr
library(data.table)
dt <- data.table(read.csv(f))
agricultureLogical <- dt$ACR == 3 & dt$AGS == 6
which(agricultureLogical)[1:3]
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg"
f <- file.path(getwd(), "jeff.jpg")
download.file(url, f, mode = "wb")
img <- readJPEG(f, native = TRUE)
quantile(img, probs = c(0.3, 0.8))
?readJPEG
library(jpeg)
install.packages("jpeg")
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg"
f <- file.path(getwd(), "jeff.jpg")
download.file(url, f, mode = "wb")
img <- readJPEG(f, native = TRUE)
quantile(img, probs = c(0.3, 0.8))
library(jpeg)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg"
f <- file.path(getwd(), "jeff.jpg")
download.file(url, f, mode = "wb")
img <- readJPEG(f, native = TRUE)
quantile(img, probs = c(0.3, 0.8))
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
f <- file.path(getwd(), "GDP.csv")
download.file(url, f)
dtGDP <- data.table(read.csv(f, skip = 4, nrows = 215))
dtGDP <- dtGDP[X != ""]
dtGDP <- dtGDP[, list(X, X.1, X.3, X.4)]
setnames(dtGDP, c("X", "X.1", "X.3", "X.4"), c("CountryCode", "rankingGDP",
"Long.Name", "gdp"))
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
f <- file.path(getwd(), "EDSTATS_Country.csv")
download.file(url, f)
dtEd <- data.table(read.csv(f))
dt <- merge(dtGDP, dtEd, all = TRUE, by = c("CountryCode"))
sum(!is.na(unique(dt$rankingGDP)))
dt
[,dt]
dt[order(rankingGDP, decreasing = TRUE), list(CountryCode, Long.Name.x, Long.Name.y,
rankingGDP, gdp)][13]
dt[, mean(rankingGDP, na.rm = TRUE), by = Income.Group]
breaks <- quantile(dt$rankingGDP, probs = seq(0, 1, 0.2), na.rm = TRUE)
dt$quantileGDP <- cut(dt$rankingGDP, breaks = breaks)
dt[Income.Group == "Lower middle income", .N, by = c("Income.Group", "quantileGDP")]
# Step1. Merges the training and the test sets to create one data set.
# setwd("C:/plodder/k0d3r/Data Science Specialization/docs/3 - Getting and Cleaning Data/Peer Assessment/")
trainData <- read.table("./data/train/X_train.txt")
dim(trainData) # 7352*561
head(trainData)
trainLabel <- read.table("./data/train/y_train.txt")
table(trainLabel)
trainSubject <- read.table("./data/train/subject_train.txt")
testData <- read.table("./data/test/X_test.txt")
dim(testData) # 2947*561
testLabel <- read.table("./data/test/y_test.txt")
table(testLabel)
testSubject <- read.table("./data/test/subject_test.txt")
joinData <- rbind(trainData, testData)
dim(joinData) # 10299*561
joinLabel <- rbind(trainLabel, testLabel)
dim(joinLabel) # 10299*1
joinSubject <- rbind(trainSubject, testSubject)
dim(joinSubject) # 10299*1
trainData <- read.table("./data/train/X_train.txt")
dim(trainData) # 7352*561
head(trainData)
trainLabel <- read.table("./data/train/y_train.txt")
table(trainLabel)
trainSubject <- read.table("./data/train/subject_train.txt")
testData <- read.table("./data/test/X_test.txt")
dim(testData) # 2947*561
testLabel <- read.table("./data/test/y_test.txt")
table(testLabel)
testSubject <- read.table("./data/test/subject_test.txt")
joinData <- rbind(trainData, testData)
dim(joinData) # 10299*561
joinLabel <- rbind(trainLabel, testLabel)
dim(joinLabel) # 10299*1
joinSubject <- rbind(trainSubject, testSubject)
dim(joinSubject) # 10299*1
setwd("C:/plodder/k0d3r/Data Science Specialization/docs/3 - Getting and Cleaning Data/Peer Assessment/
trainData <- read.table("./data/train/X_train.txt")
dim(trainData) # 7352*561
head(trainData)
trainLabel <- read.table("./data/train/y_train.txt")
table(trainLabel)
trainSubject <- read.table("./data/train/subject_train.txt")
testData <- read.table("./data/test/X_test.txt")
dim(testData) # 2947*561
testLabel <- read.table("./data/test/y_test.txt")
table(testLabel)
testSubject <- read.table("./data/test/subject_test.txt")
joinData <- rbind(trainData, testData)
dim(joinData) # 10299*561
joinLabel <- rbind(trainLabel, testLabel)
dim(joinLabel) # 10299*1
joinSubject <- rbind(trainSubject, testSubject)
dim(joinSubject) # 10299*1
setwd("C:/plodder/k0d3r/Data Science Specialization/docs/3 - Getting and Cleaning Data/Peer Assessment/")
trainData <- read.table("./data/train/X_train.txt")
dim(trainData) # 7352*561
head(trainData)
trainLabel <- read.table("./data/train/y_train.txt")
table(trainLabel)
trainSubject <- read.table("./data/train/subject_train.txt")
testData <- read.table("./data/test/X_test.txt")
dim(testData) # 2947*561
testLabel <- read.table("./data/test/y_test.txt")
table(testLabel)
testSubject <- read.table("./data/test/subject_test.txt")
joinData <- rbind(trainData, testData)
dim(joinData) # 10299*561
joinLabel <- rbind(trainLabel, testLabel)
dim(joinLabel) # 10299*1
joinSubject <- rbind(trainSubject, testSubject)
dim(joinSubject) # 10299*1
# Step2. Extracts only the measurements on the mean and standard
# deviation for each measurement.
features <- read.table("./data/features.txt")
dim(features)  # 561*2
meanStdIndices <- grep("mean\\(\\)|std\\(\\)", features[, 2])
length(meanStdIndices) # 66
joinData <- joinData[, meanStdIndices]
dim(joinData) # 10299*66
names(joinData) <- gsub("\\(\\)", "", features[meanStdIndices, 2]) # remove "()"
names(joinData) <- gsub("mean", "Mean", names(joinData)) # capitalize M
names(joinData) <- gsub("std", "Std", names(joinData)) # capitalize S
names(joinData) <- gsub("-", "", names(joinData)) # remove "-" in column names
# Step3. Uses descriptive activity names to name the activities in
# the data set
activity <- read.table("./data/activity_labels.txt")
activity[, 2] <- tolower(gsub("_", "", activity[, 2]))
substr(activity[2, 2], 8, 8) <- toupper(substr(activity[2, 2], 8, 8))
substr(activity[3, 2], 8, 8) <- toupper(substr(activity[3, 2], 8, 8))
activityLabel <- activity[joinLabel[, 1], 2]
joinLabel[, 1] <- activityLabel
names(joinLabel) <- "activity"
# Step4. Appropriately labels the data set with descriptive activity
# names.
names(joinSubject) <- "subject"
cleanedData <- cbind(joinSubject, joinLabel, joinData)
dim(cleanedData) # 10299*68
write.table(cleanedData, "merged_data.txt", row.names = FALSE) # write out the 1st dataset
# Step5. Creates a second, independent tidy data set with the average of
# each variable for each activity and each subject.
subjectLen <- length(table(joinSubject)) # 30
activityLen <- dim(activity)[1] # 6
columnLen <- dim(cleanedData)[2]
result <- matrix(NA, nrow=subjectLen*activityLen, ncol=columnLen)
result <- as.data.frame(result)
colnames(result) <- colnames(cleanedData)
row <- 1
for(i in 1:subjectLen) {
for(j in 1:activityLen) {
result[row, 1] <- sort(unique(joinSubject)[, 1])[i]
result[row, 2] <- activity[j, 2]
bool1 <- i == cleanedData$subject
bool2 <- activity[j, 2] == cleanedData$activity
result[row, 3:columnLen] <- colMeans(cleanedData[bool1&bool2, 3:columnLen])
row <- row + 1
}
}
head(result)
write.table(result, "data_with_means.txt", row.names = FALSE) # write out the 2nd dataset
trainData <- read.table("./data/train/X_train.txt")
dim(trainData) # 7352*561
head(trainData)
View(result)
View(joinLabel)
View(joinSubject)
setwd("C:/plodder/k0d3r/Data Science Specialization/docs/3 - Getting and Cleaning Data/Peer Assessment")
# STEP 1: Merges the training and the test sets to create one data set.
# Make sure the working directory is set correctly.
# In RStudio, Click Session >> Set Working Directory >> Choose Directory ...
# ...or you can set in using R script as below, for example:
# setwd("3 - Getting and Cleaning Data/Peer Assessment/")
# read the training  and test data from the six (6) text files
trainData <- read.table("./data/train/X_train.txt")
dim(trainData) # (7,352 rows; 561 columns)
trainLabel <- read.table("./data/train/y_train.txt")
table(trainLabel)
trainSubject <- read.table("./data/train/subject_train.txt")
testData <- read.table("./data/test/X_test.txt")
dim(testData) # (2,947; 561 columns)
testLabel <- read.table("./data/test/y_test.txt")
table(testLabel)
testSubject <- read.table("./data/test/subject_test.txt")
# combine the related training dataset with the related test dataset
joinData <- rbind(trainData, testData)
dim(joinData) # (10,299 rows; 561 columns)
joinLabel <- rbind(trainLabel, testLabel)
dim(joinLabel) # (10,299 rows; 1 column)
joinSubject <- rbind(trainSubject, testSubject)
dim(joinSubject) # (10,299 rows; 1 column)
# STEP 2: Extracts only the measurements on the mean and
# standard deviation for each measurement.
features <- read.table("./data/features.txt")
dim(features)  # (561 rows; 2 columns)
meanAndStandardDeviation <- grep("mean\\(\\)|std\\(\\)", features[, 2])
length(meanAndStandardDeviation) # 66 will become the number of columns
joinData <- joinData[, meanAndStandardDeviation]
dim(joinData) # (10,299 rows; 66 columns)
names(joinData) <- gsub("\\(\\)", "", features[meanAndStandardDeviation, 2]) # remove parentheses "()"
names(joinData) <- gsub("-", "", names(joinData)) # remove hyphen "-" in column names
names(joinData) <- gsub("mean", "Mean", names(joinData)) # Make Upper Case 'M'
names(joinData) <- gsub("std", "Std", names(joinData)) # Make Upper Case 'S'
# STEP 4: Appropriately labels the data set with
# descriptive activity names.
names(joinSubject) <- "subject"
tidyData <- cbind(joinSubject, joinLabel, joinData)
dim(tidyData) # (10,299 rows; 68 columns)
write.table(tidyData, "tidyData.txt", row.names = FALSE) # export the 1st dataset
# STEP 5: Creates a second, independent tidy data set with the average of
# each variable for each activity and each subject.
subjectLen <- length(table(joinSubject)) # 30 in all
activityLen <- dim(activity)[1] # 6 in all
columnLen <- dim(tidyData)[2]
result <- matrix(NA, nrow = subjectLen * activityLen, ncol = columnLen)
result <- as.data.frame(result)
colnames(result) <- colnames(tidyData)
row <- 1
for(i in 1:subjectLen) {
for(j in 1:activityLen) {
result[row, 1] <- sort(unique(joinSubject)[, 1])[i]
result[row, 2] <- activity[j, 2]
bool1 <- i == tidyData$subject
bool2 <- activity[j, 2] == tidyData$activity
result[row, 3:columnLen] <- colMeans(tidyData[bool1&bool2, 3:columnLen])
row <- row + 1
}
}
head(result)
write.table(result, "tidyDataWithMeans.txt", row.names = FALSE) # export the 2nd dataset
# data <- read.table("./tidyDataWithMeans.txt")
# data[1:12, 1:3]
